"""
å¤šæ¨¡æ€èåˆåˆ†ææ¨¡å—
å¤„ç†è¡¨æƒ…ç‰¹å¾å’Œè¯­éŸ³ç‰¹å¾çš„åˆ†æï¼Œæ”¯æŒå®æ—¶èåˆ
"""

import numpy as np
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass, field
from enum import Enum
import time
import logging

logger = logging.getLogger("TalkArena")


class EmotionType(Enum):
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    SURPRISED = "surprised"
    NERVOUS = "nervous"
    CONFIDENT = "confident"
    NEUTRAL = "neutral"
    TIRED = "tired"


class VoiceState(Enum):
    CALM = "calm"
    EXCITED = "excited"
    HESITANT = "hesitant"
    AGITATED = "agitated"
    NEUTRAL = "neutral"


@dataclass
class EmotionFeatures:
    """è¡¨æƒ…ç‰¹å¾æ•°æ®ç»“æ„"""

    eye_openness: float = 0.0
    smile_score: float = 0.0
    brow_raise: float = 0.0
    symmetry: float = 1.0
    looking_at_camera: float = 0.0
    confidence: float = 50.0
    nervousness: float = 50.0
    dominant_emotion: str = "neutral"
    head_pose: Optional[Dict] = None

    @classmethod
    def from_dict(cls, data: Dict) -> "EmotionFeatures":
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            eye_openness=data.get("eyeOpenness", 0.0),
            smile_score=data.get("smileScore", 0.0),
            brow_raise=data.get("browRaise", 0.0),
            symmetry=data.get("symmetry", 1.0),
            looking_at_camera=data.get("lookingAtCamera", 0.0),
            confidence=data.get("confidence", 50.0),
            nervousness=data.get("nervousness", 50.0),
            dominant_emotion=data.get("dominantEmotion", "neutral"),
            head_pose=data.get("headPose"),
        )

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "eyeOpenness": self.eye_openness,
            "smileScore": self.smile_score,
            "browRaise": self.brow_raise,
            "symmetry": self.symmetry,
            "lookingAtCamera": self.looking_at_camera,
            "confidence": self.confidence,
            "nervousness": self.nervousness,
            "dominantEmotion": self.dominant_emotion,
            "headPose": self.head_pose,
        }


@dataclass
class VoiceFeatures:
    """è¯­éŸ³ç‰¹å¾æ•°æ®ç»“æ„"""

    speech_rate: float = 0.0
    pitch_mean: float = 0.0
    pitch_std: float = 0.0
    volume_variance: float = 0.0
    pause_frequency: float = 0.0
    energy_pattern: str = "stable"
    voice_confidence: float = 50.0
    voice_nervousness: float = 50.0
    emotion_label: str = "neutral"

    @classmethod
    def from_dict(cls, data: Dict) -> "VoiceFeatures":
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            speech_rate=data.get("speechRate", 0.0),
            pitch_mean=data.get("pitchMean", 0.0),
            pitch_std=data.get("pitchStd", 0.0),
            volume_variance=data.get("volumeVariance", 0.0),
            pause_frequency=data.get("pauseFrequency", 0.0),
            energy_pattern=data.get("energyPattern", "stable"),
            voice_confidence=data.get("voiceConfidence", 50.0),
            voice_nervousness=data.get("voiceNervousness", 50.0),
            emotion_label=data.get("emotionLabel", "neutral"),
        )

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "speechRate": self.speech_rate,
            "pitchMean": self.pitch_mean,
            "pitchStd": self.pitch_std,
            "volumeVariance": self.volume_variance,
            "pauseFrequency": self.pause_frequency,
            "energyPattern": self.energy_pattern,
            "voiceConfidence": self.voice_confidence,
            "voiceNervousness": self.voice_nervousness,
            "emotionLabel": self.emotion_label,
        }


class MultimodalAnalyzer:
    """å¤šæ¨¡æ€åˆ†æå™¨ - åˆ†æè¡¨æƒ…å’Œè¯­éŸ³ç‰¹å¾"""

    def __init__(self):
        self.emotion_icons = {
            "happy": "ğŸ˜Š",
            "sad": "ğŸ˜¢",
            "angry": "ğŸ˜ ",
            "surprised": "ğŸ˜²",
            "nervous": "ğŸ˜°",
            "tired": "ğŸ˜´",
            "neutral": "ğŸ˜",
            "confident": "ğŸ˜",
            "calm": "ğŸ˜Œ",
            "excited": "ğŸ¤©",
            "hesitant": "ğŸ¤”",
            "agitated": "ğŸ˜¤",
        }

        self.voice_icons = {
            "calm": "ğŸµ",
            "excited": "ğŸ¸",
            "hesitant": "ğŸ“¢",
            "agitated": "ğŸ¥",
            "neutral": "ğŸ¤",
        }

    def analyze_emotion(self, features: EmotionFeatures) -> Dict:
        """åˆ†æè¡¨æƒ…ç‰¹å¾ï¼Œè¿”å›è¯„ä¼°ç»“æœ"""
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        emotion_score = 50.0

        # è‡ªä¿¡åº¦è´¡çŒ®
        emotion_score += (features.confidence - 50) * 0.3

        # ç´§å¼ åº¦æƒ©ç½š
        emotion_score -= (features.nervousness - 50) * 0.3

        # è¡¨æƒ…è‡ªç„¶åº¦
        if features.symmetry > 0.7:
            emotion_score += 10

        # çœ¼ç¥äº¤æµ
        if features.looking_at_camera > 0.6:
            emotion_score += 10

        emotion_score = max(0, min(100, emotion_score))

        # ç”Ÿæˆåé¦ˆ
        feedback = self._generate_emotion_feedback(features)

        # è·å–è¡¨æƒ…å›¾æ ‡
        emotion_icon = self.emotion_icons.get(
            features.dominant_emotion, self.emotion_icons["neutral"]
        )

        return {
            "score": round(emotion_score, 1),
            "dominant_emotion": features.dominant_emotion,
            "emotion_icon": emotion_icon,
            "confidence": round(features.confidence, 1),
            "nervousness": round(features.nervousness, 1),
            "feedback": feedback,
            "raw_features": features.to_dict(),
        }

    def analyze_voice(self, features: VoiceFeatures) -> Dict:
        """åˆ†æè¯­éŸ³ç‰¹å¾ï¼Œè¿”å›è¯„ä¼°ç»“æœ"""
        # è®¡ç®—è¯­éŸ³å¾—åˆ†
        voice_score = 50.0

        # è‡ªä¿¡åº¦è´¡çŒ®
        voice_score += (features.voice_confidence - 50) * 0.4

        # ç´§å¼ åº¦æƒ©ç½š
        voice_score -= (features.voice_nervousness - 50) * 0.4

        # è¯­é€Ÿé€‚ä¸­åŠ åˆ†
        if 2.0 <= features.speech_rate <= 4.0:
            voice_score += 10

        # éŸ³è°ƒç¨³å®šåŠ åˆ†
        if features.pitch_std < 25:
            voice_score += 10

        voice_score = max(0, min(100, voice_score))

        # ç”Ÿæˆåé¦ˆ
        feedback = self._generate_voice_feedback(features)

        # è·å–è¯­éŸ³å›¾æ ‡
        voice_icon = self.voice_icons.get(
            features.emotion_label, self.voice_icons["neutral"]
        )

        return {
            "score": round(voice_score, 1),
            "emotion_label": features.emotion_label,
            "voice_icon": voice_icon,
            "voice_confidence": round(features.voice_confidence, 1),
            "voice_nervousness": round(features.voice_nervousness, 1),
            "speech_rate": round(features.speech_rate, 1),
            "feedback": feedback,
            "raw_features": features.to_dict(),
        }

    def analyze_multimodal(
        self,
        text: str,
        emotion_features: Optional[EmotionFeatures] = None,
        voice_features: Optional[VoiceFeatures] = None,
    ) -> Dict:
        """
        ç»¼åˆåˆ†ææ–‡æœ¬ã€è¡¨æƒ…å’Œè¯­éŸ³
        è¿”å›å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        results = {
            "text_score": 50.0,  # è¿™é‡Œå¯ä»¥è°ƒç”¨LLMè¯„ä¼°æ–‡æœ¬
            "emotion_analysis": None,
            "voice_analysis": None,
            "overall_score": 50.0,
            "feedback": "",
            "inconsistencies": [],
            "suggestions": [],
        }

        weights = {"text": 0.5, "emotion": 0.25, "voice": 0.25}
        scores = {"text": 50.0, "emotion": 50.0, "voice": 50.0}

        # åˆ†æè¡¨æƒ…
        if emotion_features:
            emotion_result = self.analyze_emotion(emotion_features)
            results["emotion_analysis"] = emotion_result
            scores["emotion"] = emotion_result["score"]
        else:
            # å¦‚æœæ²¡æœ‰è¡¨æƒ…æ•°æ®ï¼Œé™ä½è¡¨æƒ…æƒé‡
            weights["text"] += weights["emotion"] * 0.5
            weights["voice"] += weights["emotion"] * 0.5
            weights["emotion"] = 0

        # åˆ†æè¯­éŸ³
        if voice_features:
            voice_result = self.analyze_voice(voice_features)
            results["voice_analysis"] = voice_result
            scores["voice"] = voice_result["score"]
        else:
            # å¦‚æœæ²¡æœ‰è¯­éŸ³æ•°æ®ï¼Œé™ä½è¯­éŸ³æƒé‡
            weights["text"] += weights["voice"]
            weights["voice"] = 0

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = sum(scores[k] * weights[k] for k in scores)
        results["overall_score"] = round(overall_score, 1)
        results["breakdown"] = {
            "text": round(scores["text"], 1),
            "emotion": round(scores["emotion"], 1),
            "voice": round(scores["voice"], 1),
        }

        # æ£€æµ‹ä¸ä¸€è‡´æ€§
        inconsistencies = self._detect_inconsistencies(
            text, emotion_features, voice_features
        )
        results["inconsistencies"] = inconsistencies

        # ç”Ÿæˆç»¼åˆåé¦ˆ
        results["feedback"] = self._generate_overall_feedback(results, inconsistencies)

        # ç”Ÿæˆå»ºè®®
        results["suggestions"] = self._generate_suggestions(
            emotion_features, voice_features
        )

        return results

    def _generate_emotion_feedback(self, features: EmotionFeatures) -> str:
        """ç”Ÿæˆè¡¨æƒ…åé¦ˆ"""
        feedbacks = []

        if features.nervousness > 70:
            feedbacks.append("è¡¨æƒ…ç•¥æ˜¾ç´§å¼ ")
        elif features.nervousness < 30:
            feedbacks.append("ç¥æ€è‡ªè‹¥")

        if features.confidence > 70:
            feedbacks.append("è‡ªä¿¡æ»¡æ»¡")
        elif features.confidence < 40:
            feedbacks.append("å¯ä»¥æ›´è‡ªä¿¡äº›")

        if features.looking_at_camera > 0.7:
            feedbacks.append("çœ¼ç¥äº¤æµå……åˆ†")
        elif features.looking_at_camera < 0.3:
            feedbacks.append("å»ºè®®å¤šè¿›è¡Œçœ¼ç¥äº¤æµ")

        if features.symmetry < 0.6:
            feedbacks.append("é¢éƒ¨è¡¨æƒ…è‡ªç„¶åº¦å¯ä»¥æå‡")

        return " | ".join(feedbacks) if feedbacks else "è¡¨æƒ…ç®¡ç†åˆ°ä½"

    def _generate_voice_feedback(self, features: VoiceFeatures) -> str:
        """ç”Ÿæˆè¯­éŸ³åé¦ˆ"""
        feedbacks = []

        if features.voice_nervousness > 70:
            feedbacks.append("å£°éŸ³ç•¥æ˜¾ç´§å¼ ")
        elif features.voice_nervousness < 30:
            feedbacks.append("å£°éŸ³æ²‰ç¨³")

        if features.voice_confidence > 70:
            feedbacks.append("è¯­æ°”åšå®šæœ‰åŠ›")

        if features.speech_rate > 4.5:
            feedbacks.append("è¯­é€Ÿåå¿«")
        elif features.speech_rate < 2.0:
            feedbacks.append("è¯­é€Ÿåæ…¢")
        else:
            feedbacks.append("è¯­é€Ÿé€‚ä¸­")

        if features.pitch_std > 30:
            feedbacks.append("éŸ³è°ƒæ³¢åŠ¨è¾ƒå¤§")
        elif features.pitch_std < 15:
            feedbacks.append("éŸ³è°ƒå¹³ç¨³")

        if features.pause_frequency > 0.2:
            feedbacks.append("åœé¡¿è¾ƒå¤š")

        return " | ".join(feedbacks) if feedbacks else "è¯­æ°”å¾—å½“"

    def _detect_inconsistencies(
        self,
        text: str,
        emotion_features: Optional[EmotionFeatures],
        voice_features: Optional[VoiceFeatures],
    ) -> list:
        """æ£€æµ‹å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§"""
        inconsistencies = []

        # æ£€æµ‹1: æ–‡æœ¬å¼€å¿ƒä½†è¡¨æƒ…ä¸ç¬‘
        if emotion_features and ("å¼€å¿ƒ" in text or "é«˜å…´" in text or "è°¢è°¢" in text):
            if emotion_features.smile_score < 0.2:
                inconsistencies.append("å˜´ä¸Šè¯´å¼€å¿ƒï¼Œä½†è¡¨æƒ…æœªè§ç¬‘å®¹")

        # æ£€æµ‹2: æ–‡æœ¬å¼ºç¡¬ä½†å£°éŸ³é¢¤æŠ–
        if voice_features and any(
            word in text for word in ["å¿…é¡»", "ä¸€å®š", "è‚¯å®š", "æ²¡é”™"]
        ):
            if voice_features.voice_nervousness > 60:
                inconsistencies.append("è¯è¯­å¼ºç¡¬ä½†å£°éŸ³ç•¥æ˜¾ç´§å¼ ")

        # æ£€æµ‹3: æ–‡æœ¬è°¦è™šä½†è¡¨æƒ…å‚²æ…¢
        if emotion_features and any(
            word in text for word in ["ä¸æ•¢", "æƒ­æ„§", "è¿‡å¥–", "å“ªé‡Œ"]
        ):
            if emotion_features.brow_raise > 0.7:
                inconsistencies.append("å˜´ä¸Šè°¦è™šä½†è¡¨æƒ…æ˜¾å¾—é«˜å‚²")

        # æ£€æµ‹4: è¡¨æƒ…ç´§å¼ ä½†å£°éŸ³è‡ªä¿¡
        if emotion_features and voice_features:
            if (
                emotion_features.nervousness > 60
                and voice_features.voice_confidence > 70
            ):
                inconsistencies.append("è¡¨æƒ…ç´§å¼ ä½†å£°éŸ³å¾ˆæ²‰ç¨³")

        return inconsistencies

    def _generate_overall_feedback(self, results: Dict, inconsistencies: list) -> str:
        """ç”Ÿæˆç»¼åˆåé¦ˆ"""
        score = results["overall_score"]

        if score >= 80:
            base_feedback = "è¡¨ç°å‡ºè‰²ï¼æ°”åœºå¾ˆè¶³"
        elif score >= 60:
            base_feedback = "è¡¨ç°ä¸é”™ï¼Œè¿˜æœ‰æå‡ç©ºé—´"
        elif score >= 40:
            base_feedback = "è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦å¤šåŠ ç»ƒä¹ "
        else:
            base_feedback = "å»ºè®®è°ƒæ•´å¿ƒæ€ï¼Œæ”¾æ¾ä¸€äº›"

        # æ·»åŠ ä¸ä¸€è‡´æ€§è­¦å‘Š
        if inconsistencies:
            base_feedback += f"ï¼ˆæ³¨æ„ï¼š{inconsistencies[0]}ï¼‰"

        return base_feedback

    def _generate_suggestions(
        self,
        emotion_features: Optional[EmotionFeatures],
        voice_features: Optional[VoiceFeatures],
    ) -> list:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        if emotion_features:
            if emotion_features.nervousness > 60:
                suggestions.append("ğŸ¯ å»ºè®®ï¼šå¯¹ç€é•œå­ç»ƒä¹ å¾®ç¬‘ï¼Œæ·±å‘¼å¸æ”¾æ¾")
            if emotion_features.looking_at_camera < 0.4:
                suggestions.append("ğŸ‘€ å»ºè®®ï¼šè¯´è¯æ—¶è¦çœ‹ç€å¯¹æ–¹ï¼Œå¢å¼ºçœ¼ç¥äº¤æµ")
            if emotion_features.symmetry < 0.7:
                suggestions.append("ğŸ˜Š å»ºè®®ï¼šè®©è¡¨æƒ…æ›´è‡ªç„¶ï¼Œæ”¾æ¾é¢éƒ¨è‚Œè‚‰")

        if voice_features:
            if voice_features.voice_nervousness > 60:
                suggestions.append("ğŸ—£ï¸ å»ºè®®ï¼šæ”¾æ…¢è¯­é€Ÿï¼Œç”¨è…¹å¼å‘¼å¸ç¨³å®šå£°éŸ³")
            if voice_features.pause_frequency > 0.2:
                suggestions.append('ğŸ’¬ å»ºè®®ï¼šå‡å°‘"å—¯""å•Š"ç­‰å£å¤´ç¦…')
            if voice_features.speech_rate > 4.5:
                suggestions.append("â±ï¸ å»ºè®®ï¼šæ”¾æ…¢è¯­é€Ÿï¼Œç»™è‡ªå·±æ€è€ƒçš„æ—¶é—´")

        return suggestions

    def get_status_icons(
        self,
        emotion_features: Optional[EmotionFeatures] = None,
        voice_features: Optional[VoiceFeatures] = None,
    ) -> Dict[str, str]:
        """
        è·å–çŠ¶æ€å›¾æ ‡ï¼Œç”¨äºå‰ç«¯å±•ç¤º
        è¿”å›è¡¨æƒ…å›¾æ ‡å’Œè¯­éŸ³å›¾æ ‡
        """
        icons = {
            "emotion_icon": "â“",
            "emotion_status": "æœªæ£€æµ‹",
            "voice_icon": "â“",
            "voice_status": "æœªæ£€æµ‹",
        }

        if emotion_features:
            emotion_icon = self.emotion_icons.get(
                emotion_features.dominant_emotion, "ğŸ˜"
            )
            icons["emotion_icon"] = emotion_icon
            icons["emotion_status"] = self._get_emotion_status_text(emotion_features)

        if voice_features:
            voice_icon = self.voice_icons.get(voice_features.emotion_label, "ğŸ¤")
            icons["voice_icon"] = voice_icon
            icons["voice_status"] = self._get_voice_status_text(voice_features)

        return icons

    def _get_emotion_status_text(self, features: EmotionFeatures) -> str:
        """è·å–è¡¨æƒ…çŠ¶æ€æ–‡æœ¬"""
        emotion_map = {
            "happy": "å¼€å¿ƒ",
            "sad": "éš¾è¿‡",
            "angry": "ç”Ÿæ°”",
            "surprised": "æƒŠè®¶",
            "nervous": "ç´§å¼ ",
            "tired": "ç–²æƒ«",
            "neutral": "å¹³é™",
            "confident": "è‡ªä¿¡",
        }

        emotion_text = emotion_map.get(
            features.dominant_emotion, features.dominant_emotion
        )

        # æ·»åŠ è‡ªä¿¡åº¦/ç´§å¼ åº¦æç¤º
        if features.confidence > 70:
            return f"{emotion_text}Â·è‡ªä¿¡"
        elif features.nervousness > 60:
            return f"{emotion_text}Â·ç´§å¼ "

        return emotion_text

    def _get_voice_status_text(self, features: VoiceFeatures) -> str:
        """è·å–è¯­éŸ³çŠ¶æ€æ–‡æœ¬"""
        emotion_map = {
            "calm": "æ²‰ç¨³",
            "excited": "æ¿€åŠ¨",
            "hesitant": "çŠ¹è±«",
            "agitated": "ç„¦èº",
            "neutral": "å¹³å’Œ",
        }

        voice_text = emotion_map.get(features.emotion_label, features.emotion_label)

        # æ·»åŠ è¯­é€Ÿæç¤º
        if features.speech_rate > 4.5:
            return f"{voice_text}Â·åå¿«"
        elif features.speech_rate < 2.0:
            return f"{voice_text}Â·åæ…¢"

        return voice_text
