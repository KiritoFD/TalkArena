"""
å‡çº§ç‰ˆå¤šæ¨¡æ€èåˆåˆ†ææ¨¡å—
æ”¯æŒå¾®è¡¨æƒ…åˆ†æå’Œè¯­éŸ³æƒ…æ„Ÿåˆ†æ
"""

import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import logging

from core.emotion_state import (
    MicroExpressionFeatures,
    VoiceEmotionFeatures,
    MultimodalEmotionState,
    UserEmotionStateMachine,
    EmotionMemory,
)

logger = logging.getLogger("TalkArena")


class VoiceEmotionAnalyzer:
    """è¯­éŸ³æƒ…æ„Ÿåˆ†æå™¨"""

    def __init__(self):
        self.sample_rate = 16000

    def analyze(self, audio_data: np.ndarray) -> VoiceEmotionFeatures:
        """åˆ†æè¯­éŸ³æƒ…æ„Ÿç‰¹å¾"""

        features = VoiceEmotionFeatures()

        features.loudness = float(np.mean(np.abs(audio_data)))

        zero_crossings = np.sum(np.abs(np.diff(np.sign(audio_data)))) / 2
        speech_rate = (zero_crossings / self.sample_rate) * 60
        features.speech_rate = min(6.0, max(1.0, speech_rate / 100))

        features.energy_variance = float(np.var(np.abs(audio_data)))

        features.emotion_scores = self._infer_emotion(features)

        features.valence = self._compute_valence(features)
        features.arousal = self._compute_arousal(features)
        features.dominance = self._compute_dominance(features)

        return features

    def _infer_emotion(self, features: VoiceEmotionFeatures) -> Dict[str, float]:
        """æ¨æ–­è¯­éŸ³æƒ…æ„Ÿ"""
        scores = {}

        loudness = features.loudness
        speech_rate = features.speech_rate
        energy_var = features.energy_variance

        scores["happy"] = (
            min(1.0, loudness * 1.2) * 0.3 + min(1.0, speech_rate / 4.0) * 0.3
        )
        scores["sad"] = (1 - loudness) * 0.4 + (1 - min(1.0, speech_rate / 3.0)) * 0.3
        scores["angry"] = min(1.0, loudness) * 0.4 + min(1.0, energy_var * 10) * 0.3
        scores["nervous"] = (
            min(1.0, energy_var * 15) * 0.5 + (1 - min(1.0, speech_rate / 2.5)) * 0.3
        )
        scores["confident"] = (1 - energy_var * 8) * 0.4 + min(
            1.0, speech_rate / 3.5
        ) * 0.3
        scores["hesitant"] = (1 - loudness) * 0.3 + min(1.0, energy_var * 8) * 0.4

        for k in scores:
            scores[k] = max(0.0, min(1.0, scores[k]))

        return scores

    def _compute_valence(self, f: VoiceEmotionFeatures) -> float:
        scores = f.emotion_scores or {}
        return (scores.get("happy", 0) - scores.get("sad", 0)) * 0.5

    def _compute_arousal(self, f: VoiceEmotionFeatures) -> float:
        scores = f.emotion_scores or {}
        return max(
            scores.get("angry", 0),
            scores.get("nervous", 0),
            scores.get("happy", 0) * 0.8,
        )

    def _compute_dominance(self, f: VoiceEmotionFeatures) -> float:
        scores = f.emotion_scores or {}
        return (
            scores.get("confident", 0.5) * 0.7 + (1 - scores.get("hesitant", 0)) * 0.3
        )


class MultimodalFusionEngine:
    """å¤šæ¨¡æ€èåˆå¼•æ“"""

    def __init__(self):
        self.state_machine = UserEmotionStateMachine()
        self.emotion_memory = EmotionMemory()
        self.voice_analyzer = VoiceEmotionAnalyzer()

    def process_face_features(self, face_data: Dict) -> MultimodalEmotionState:
        """å¤„ç†è¡¨æƒ…ç‰¹å¾"""
        face_features = MicroExpressionFeatures.from_dict(face_data)

        state = self.state_machine.update(face_features=face_features)

        return state

    def process_voice_features(self, voice_data: Dict) -> MultimodalEmotionState:
        """å¤„ç†è¯­éŸ³ç‰¹å¾"""
        voice_features = VoiceEmotionFeatures.from_dict(voice_data)

        state = self.state_machine.update(voice_features=voice_features)

        return state

    def fuse(
        self,
        face_data: Optional[Dict] = None,
        voice_data: Optional[Dict] = None,
        text: str = "",
    ) -> MultimodalEmotionState:
        """èåˆå¤šæ¨¡æ€ç‰¹å¾"""

        face_features = None
        voice_features = None

        if face_data:
            face_features = MicroExpressionFeatures.from_dict(face_data)

        if voice_data:
            voice_features = VoiceEmotionFeatures.from_dict(voice_data)

        text_sentiment = self._analyze_text_sentiment(text) if text else None

        state = self.state_machine.update(
            face_features=face_features,
            voice_features=voice_features,
            text_sentiment=text_sentiment,
        )

        return state

    def _analyze_text_sentiment(self, text: str) -> Dict:
        """ç®€å•çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æ"""
        text_lower = text.lower()

        positive_words = ["å¼€å¿ƒ", "é«˜å…´", "è°¢è°¢", "å¥½çš„", "å¯ä»¥", "æ²¡é—®é¢˜"]
        negative_words = ["ä¸è¡Œ", "ä¸è¦", "ç”Ÿæ°”", "æ„¤æ€’", "éš¾è¿‡", "ç´§å¼ "]

        positive_count = sum(1 for w in positive_words if w in text_lower)
        negative_count = sum(1 for w in negative_words if w in text_lower)

        if positive_count > negative_count:
            return {"sentiment": "positive", "confidence": 0.6}
        elif negative_count > positive_count:
            return {"sentiment": "negative", "confidence": 0.6}

        return {"sentiment": "neutral", "confidence": 0.3}

    def store_interaction(
        self,
        user_input: str,
        multimodal_state: MultimodalEmotionState,
        npc_response: str,
    ):
        """å­˜å‚¨äº¤äº’åˆ°è®°å¿†"""
        self.emotion_memory.store(
            user_input=user_input,
            multimodal_state=multimodal_state,
            npc_response=npc_response,
        )

    def get_related_memories(
        self, current_state: MultimodalEmotionState, top_k: int = 3
    ) -> List[Dict]:
        """è·å–ç›¸å…³è®°å¿†"""
        return self.emotion_memory.retrieve(
            current_input="", current_emotion=current_state, top_k=top_k
        )

    def get_emotion_patterns(self) -> Dict:
        """è·å–ç”¨æˆ·æƒ…æ„Ÿæ¨¡å¼"""
        return self.emotion_memory.get_emotion_patterns()

    def get_trend(self) -> str:
        """è·å–æƒ…æ„Ÿè¶‹åŠ¿"""
        return self.state_machine.get_trend()

    def get_history(self, last_n: int = 10) -> List[Dict]:
        """è·å–æƒ…æ„Ÿå†å²"""
        return self.state_machine.get_history(last_n)


class EmotionDrivenResponseGenerator:
    """æƒ…æ„Ÿé©±åŠ¨å“åº”ç”Ÿæˆå™¨ - ä¸ºNPCç”Ÿæˆè¡Œä¸ºæŒ‡ä»¤"""

    def __init__(self):
        self.behavior_templates = self._load_templates()

    def _load_templates(self) -> Dict:
        return {
            "confident": {
                "facial": "ä¸¥è‚ƒç›´è§†",
                "eye_contact": "åšå®š",
                "body_posture": "å‰å€¾",
                "voice_tone": "æœ‰åŠ›",
                "gesture": "æŒ‡ç‚¹",
            },
            "nervous": {
                "facial": "å…³åˆ‡",
                "eye_contact": "æ¸©å’Œ",
                "body_posture": "æ”¾æ¾",
                "voice_tone": "ç¼“å’Œ",
                "gesture": "å®‰æŠš",
            },
            "angry": {
                "facial": "æƒŠè®¶",
                "eye_contact": "å…³åˆ‡",
                "body_posture": "æš‚åœ",
                "voice_tone": "ç¼“å’Œ",
                "gesture": "ä¸¾æ‰‹",
            },
            "happy": {
                "facial": "å¼€å¿ƒ",
                "eye_contact": "æ˜äº®",
                "body_posture": "æ”¾æ¾",
                "voice_tone": "è½»å¿«",
                "gesture": "ç‚¹å¤´",
            },
            "sad": {
                "facial": "åŒæƒ…",
                "eye_contact": "æ¸©æŸ”",
                "body_posture": "å‰å€¾",
                "voice_tone": "æ¸©å’Œ",
                "gesture": "è½»æ‹",
            },
            "neutral": {
                "facial": "è‡ªç„¶",
                "eye_contact": "æ­£å¸¸",
                "body_posture": "è‡ªç„¶",
                "voice_tone": "æ­£å¸¸",
                "gesture": "æ— ",
            },
        }

    def generate_behavior_cues(
        self, emotion_state: MultimodalEmotionState, npc_personality: str = "default"
    ) -> Dict:
        """ç”Ÿæˆè¡Œä¸ºæç¤º"""

        primary = emotion_state.primary_emotion
        template = self.behavior_templates.get(
            primary, self.behavior_templates["neutral"]
        )

        intensity = emotion_state.emotion_intensity

        cues = {
            "facial_expression": template["facial"],
            "eye_contact": template["eye_contact"],
            "body_language": template["body_posture"],
            "voice_tone": template["voice_tone"],
            "hand_gesture": template["gesture"],
            "intensity": intensity,
            "emotion": primary,
            "hidden_sentiment": emotion_state.hidden_sentiment,
            "confidence": emotion_state.confidence,
        }

        if emotion_state.inconsistencies:
            cues["inconsistencies"] = emotion_state.inconsistencies

        return cues

    def get_npc_strategy(
        self, user_emotion: MultimodalEmotionState, npc_role: str = "aggressor"
    ) -> Dict:
        """è·å–NPCç­–ç•¥å»ºè®®"""

        emotion = user_emotion.primary_emotion

        strategies = {
            "confident": {
                "aggressor": {
                    "tactic": "defensive_counter",
                    "description": "ç”¨æˆ·è‡ªä¿¡ï¼Œè¦æ›´åŠ è°¨æ…ï¼Œå¢åŠ éš¾åº¦",
                    "tone": "è®¤çœŸ",
                    "emotional_tone": "ä¸¥è‚ƒ",
                },
                "supporter": {
                    "tactic": "observation",
                    "description": "è§‚å¯Ÿå­¦ä¹ ",
                    "tone": "æ¸©å’Œ",
                    "emotional_tone": "ä¸­æ€§",
                },
            },
            "nervous": {
                "aggressor": {
                    "tactic": "continual_attack",
                    "description": "ç”¨æˆ·ç´§å¼ ï¼Œç»§ç»­æ–½å‹",
                    "tone": "ä¸¥å‰",
                    "emotional_tone": "å¼ºåŠ¿",
                },
                "supporter": {
                    "tactic": "give_way",
                    "description": "é€‚å½“ç»™å°é˜¶",
                    "tone": "æ¸©å’Œ",
                    "emotional_tone": "å…³åˆ‡",
                },
            },
            "angry": {
                "aggressor": {
                    "tactic": "de_escalation",
                    "description": "ç”¨æˆ·æ„¤æ€’ï¼Œé€‚å½“æ”¶æ•›",
                    "tone": "ç¼“å’Œ",
                    "emotional_tone": "æ”¶æ•›",
                },
                "supporter": {
                    "tactic": "support",
                    "description": "æ”¯æŒå®‰æŠš",
                    "tone": "æ¸©å’Œ",
                    "emotional_tone": "å…³å¿ƒ",
                },
            },
        }

        default = {
            "tactic": "normal",
            "description": "æ­£å¸¸åº”å¯¹",
            "tone": "è‡ªç„¶",
            "emotional_tone": "ä¸­æ€§",
        }

        return strategies.get(emotion, {}).get(npc_role, default)


class MultimodalAnalyzer:
    """ä¸»å¤šæ¨¡æ€åˆ†æå™¨ - å…¼å®¹æ—§æ¥å£"""

    def __init__(self):
        self.fusion_engine = MultimodalFusionEngine()
        self.response_generator = EmotionDrivenResponseGenerator()

    def analyze_multimodal(
        self,
        text: str = "",
        emotion_features: Optional[Dict] = None,
        voice_features: Optional[Dict] = None,
    ) -> Dict:
        """åˆ†æå¤šæ¨¡æ€è¾“å…¥"""

        state = self.fusion_engine.fuse(
            face_data=emotion_features, voice_data=voice_features, text=text
        )

        behavior_cues = self.response_generator.generate_behavior_cues(state)

        patterns = self.fusion_engine.get_emotion_patterns()
        trend = self.fusion_engine.get_trend()

        return {
            "emotion_state": state.to_dict(),
            "behavior_cues": behavior_cues,
            "patterns": patterns,
            "trend": trend,
            "inconsistencies": state.inconsistencies,
        }

    def process_turn(self, user_input: str, multimodal_data: Dict) -> Dict:
        """å¤„ç†ä¸€è½®äº¤äº’"""

        emotion_data = multimodal_data.get("emotion", {})
        voice_level = multimodal_data.get("voice_level", 0)

        face_data = None
        if emotion_data:
            face_data = {
                "confidence": emotion_data.get("confidence", 0.5),
                "nervousness": emotion_data.get("nervous", 0.5),
                "calm": emotion_data.get("calm", 0.5),
                "focus": emotion_data.get("focus", 0.5),
                "happiness": emotion_data.get("confidence", 0.5) * 0.3,
                "sadness": emotion_data.get("nervous", 0.5) * 0.3,
                "anger": 0.0,
                "valence": emotion_data.get("confidence", 0.5) - 0.5,
                "arousal": emotion_data.get("nervous", 0.5),
                "dominance": emotion_data.get("confidence", 0.5),
                "smileGenuineScore": emotion_data.get("confidence", 0.5) * 0.3,
                "browTension": emotion_data.get("nervous", 0.5) * 0.3,
            }

        voice_data = None
        if voice_level > 0:
            voice_data = {
                "loudness": voice_level / 100.0,
                "speechRate": 3.0,
                "pitchMean": 150.0,
                "pitchStd": 20.0,
                "energyVariance": 0.1,
                "emotionScores": {
                    "nervous": (100 - voice_level) / 100.0 * 0.3,
                    "confident": voice_level / 100.0 * 0.5,
                },
            }

        result = self.analyze_multimodal(
            text=user_input, emotion_features=face_data, voice_features=voice_data
        )

        return result

    def store_memory(self, user_input: str, multimodal_data: Dict, npc_response: str):
        """å­˜å‚¨äº¤äº’è®°å¿†"""

        state = self.fusion_engine.state_machine.current_state

        from core.emotion_state import MultimodalEmotionState

        temp_state = MultimodalEmotionState(
            primary_emotion=state,
            emotion_intensity=0.5,
            valence=0.0,
            arousal=0.5,
            dominance=0.5,
        )

        self.fusion_engine.store_interaction(user_input, temp_state, npc_response)

    def get_status_icons(
        self, emotion_features=None, voice_features=None
    ) -> Dict[str, str]:
        """è·å–çŠ¶æ€å›¾æ ‡"""

        state = self.fusion_engine.state_machine

        emotion_icons = {
            "confident": "ğŸ˜",
            "nervous": "ğŸ˜°",
            "angry": "ğŸ˜ ",
            "happy": "ğŸ˜Š",
            "sad": "ğŸ˜¢",
            "surprised": "ğŸ˜²",
            "confused": "ğŸ˜•",
            "contemptuous": "ğŸ™„",
            "neutral": "ğŸ˜",
        }

        return {
            "emotion_icon": emotion_icons.get(state.current_state, "ğŸ˜"),
            "emotion_status": state.current_state,
            "confidence": f"{state.state_confidence:.0%}",
        }
